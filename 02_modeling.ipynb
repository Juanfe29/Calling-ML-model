{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f77e62",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58018bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, f1_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4328b55",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Configure\n",
    "OUTPUT_DIR = \"analysis\"\n",
    "DATA_FILE = \"bank-additional-full.csv\"\n",
    "RESULTS_FILE = os.path.join(OUTPUT_DIR, \"modeling_results.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e94b910",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    return pd.read_csv(filepath, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642df352",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # 1. Drop 'duration' (Leakage)\n",
    "    if 'duration' in df.columns:\n",
    "        print(\"Dropping 'duration' column (Leakage prevention)...\")\n",
    "        df = df.drop(columns=['duration'])\n",
    "    \n",
    "    # 2. Feature Engineering\n",
    "    print(\"Performing Feature Engineering...\")\n",
    "    \n",
    "    # 'pdays': Create binary 'was_contacted'\n",
    "    # 999 means client was not previously contacted\n",
    "    if 'pdays' in df.columns:\n",
    "        df['was_contacted'] = (df['pdays'] < 999).astype(int)\n",
    "        df = df.drop(columns=['pdays'])\n",
    "    \n",
    "    # 'age': Create 'is_retired' (> 60)\n",
    "    if 'age' in df.columns:\n",
    "        df['is_retired'] = (df['age'] > 60).astype(int)\n",
    "    \n",
    "    # 3. Encode Target\n",
    "    le = LabelEncoder()\n",
    "    df['y'] = le.fit_transform(df['y']) # yes=1, no=0\n",
    "    \n",
    "    # 4. Identify Features\n",
    "    target = 'y'\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    \n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(f\"Categorical columns: {categorical_cols}\")\n",
    "    print(f\"Numerical columns: {numerical_cols}\")\n",
    "    \n",
    "    # 5. Create Preprocessing Pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ])\n",
    "    \n",
    "    return X, y, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1eb8c5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name, f):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    f.write(f\"## {model_name} Results\\n\\n\")\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    f.write(\"### Classification Report\\n\")\n",
    "    f.write(f\"```\\n{report}\\n```\\n\\n\")\n",
    "    \n",
    "    # Explicit F1-Score for Class 1\n",
    "    f1_class1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "    f.write(f\"- **F1-Score (Class 1):** {f1_class1:.4f}\\n\\n\")\n",
    "    \n",
    "    # ROC-AUC\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    f.write(f\"- **ROC AUC Score:** {auc:.4f}\\n\\n\")\n",
    "    \n",
    "    # Precision-Recall Curve (NEW)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, marker='.', label=model_name)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f\"pr_curve_{model_name.replace(' ', '_')}.png\"))\n",
    "    plt.close()\n",
    "    f.write(f\"![Precision-Recall Curve {model_name}](pr_curve_{model_name.replace(' ', '_')}.png)\\n\\n\")\n",
    "    \n",
    "    # Confusion Matrix Plot\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f\"cm_{model_name.replace(' ', '_')}.png\"))\n",
    "    plt.close()\n",
    "    f.write(f\"![Confusion Matrix {model_name}](cm_{model_name.replace(' ', '_')}.png)\\n\\n\")\n",
    "\n",
    "    # Feature Importance (if available)\n",
    "    if hasattr(model, 'named_steps'):\n",
    "        classifier = model.named_steps['classifier']\n",
    "        # Handle XGBoost feature importances\n",
    "        if hasattr(classifier, 'feature_importances_'):\n",
    "            preprocessor = model.named_steps['preprocessor']\n",
    "            try:\n",
    "                cat_names = preprocessor.named_transformers_['cat'].get_feature_names_out()\n",
    "                num_names = preprocessor.named_transformers_['num'].feature_names_in_\n",
    "                feature_names = np.concatenate([num_names, cat_names])\n",
    "                \n",
    "                importances = classifier.feature_importances_\n",
    "                # Ensure lengths match (sometimes OneHot drops columns)\n",
    "                if len(importances) == len(feature_names):\n",
    "                    indices = np.argsort(importances)[::-1][:20] # Top 20\n",
    "                    \n",
    "                    plt.figure(figsize=(10, 8))\n",
    "                    plt.title(f\"Feature Importances - {model_name} (Top 20)\")\n",
    "                    plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "                    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "                    plt.gca().invert_yaxis()\n",
    "                    plt.xlabel('Relative Importance')\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(os.path.join(OUTPUT_DIR, f\"fi_{model_name.replace(' ', '_')}.png\"))\n",
    "                    plt.close()\n",
    "                    f.write(f\"![Feature Importance {model_name}](fi_{model_name.replace(' ', '_')}.png)\\n\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not plot feature importance for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab93c5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        print(f\"Error: {DATA_FILE} not found.\")\n",
    "        return\n",
    "\n",
    "    df = load_data(DATA_FILE)\n",
    "    X, y, preprocessor = preprocess_data(df)\n",
    "    \n",
    "    # Split Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Calculate scale_pos_weight for XGBoost\n",
    "    # sum(negative) / sum(positive)\n",
    "    pos_count = y_train.sum()\n",
    "    neg_count = len(y_train) - pos_count\n",
    "    scale_pos_weight = neg_count / pos_count\n",
    "    print(f\"XGBoost scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "    with open(RESULTS_FILE, \"w\") as f:\n",
    "        f.write(\"# Modeling Results\\n\\n\")\n",
    "        \n",
    "        # 1. Logistic Regression (Baseline)\n",
    "        lr_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42))\n",
    "        ])\n",
    "        \n",
    "        lr_pipeline.fit(X_train, y_train)\n",
    "        evaluate_model(lr_pipeline, X_test, y_test, \"Logistic Regression\", f)\n",
    "        \n",
    "        # 2. Random Forest\n",
    "        rf_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "        \n",
    "        rf_pipeline.fit(X_train, y_train)\n",
    "        evaluate_model(rf_pipeline, X_test, y_test, \"Random Forest\", f)\n",
    "        \n",
    "        # 3. XGBoost\n",
    "        xgb_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                scale_pos_weight=scale_pos_weight,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                eval_metric='logloss'\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        xgb_pipeline.fit(X_train, y_train)\n",
    "        evaluate_model(xgb_pipeline, X_test, y_test, \"XGBoost\", f)\n",
    "\n",
    "    print(f\"Results saved to {RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
